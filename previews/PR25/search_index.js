var documenterSearchIndex = {"docs":
[{"location":"normalkernel/#NormalKernel","page":"NormalKernel","title":"NormalKernel","text":"","category":"section"},{"location":"normalkernel/","page":"NormalKernel","title":"NormalKernel","text":"CurrentModule = MarkovKernels","category":"page"},{"location":"normalkernel/","page":"NormalKernel","title":"NormalKernel","text":"The Normal kernel is denoted by","category":"page"},{"location":"normalkernel/","page":"NormalKernel","title":"NormalKernel","text":"k(ymid x) = mathcalN(y  mu(x)   Sigma )","category":"page"},{"location":"normalkernel/","page":"NormalKernel","title":"NormalKernel","text":"As with the Normal distributions, the explicit expression on the kernel depends on whether it is real or complex valued.","category":"page"},{"location":"normal/#Normal","page":"Normal distributions","title":"Normal","text":"","category":"section"},{"location":"normal/","page":"Normal distributions","title":"Normal distributions","text":"CurrentModule = MarkovKernels","category":"page"},{"location":"normal/","page":"Normal distributions","title":"Normal distributions","text":"The standard parametrisation of the Normal distribution is given by","category":"page"},{"location":"normal/","page":"Normal distributions","title":"Normal distributions","text":"mathcalN(x  mu   Sigma )","category":"page"},{"location":"normal/","page":"Normal distributions","title":"Normal distributions","text":"where mu is the mean vector and Sigma is the covariance matrix. The exact expression for the probabiltiy density function depends on whether x is vector with real or complex values, both are supported. For real valued vectors the density function is given by","category":"page"},{"location":"normal/","page":"Normal distributions","title":"Normal distributions","text":"mathcalN(x  mu   Sigma ) = 2pi Sigma^-12 exp Big(  -frac12(x-mu)^* Sigma^-1 (x-mu)  Big)","category":"page"},{"location":"normal/","page":"Normal distributions","title":"Normal distributions","text":"whereas for complex valued vectors the density function is given by","category":"page"},{"location":"normal/","page":"Normal distributions","title":"Normal distributions","text":"mathcalN(x  mu   Sigma ) = pi Sigma^-1 exp Big(  -(x-mu)^* Sigma^-1 (x-mu)  Big)","category":"page"},{"location":"normal/","page":"Normal distributions","title":"Normal distributions","text":"Types:","category":"page"},{"location":"normal/","page":"Normal distributions","title":"Normal distributions","text":"abstract type AbstractNormal{T<:Number}  <: AbstractDistribution end # normal distributions with realisations in real / complex Euclidean spaces\nNormal{T} <: AbstractNormal{T} # mean vector / covariance matrix parametrisation of normal distributions","category":"page"},{"location":"normal/","page":"Normal distributions","title":"Normal distributions","text":"Functionality:","category":"page"},{"location":"normal/","page":"Normal distributions","title":"Normal distributions","text":"dim(N::AbstractNormal)  # dimension  of the normal distribution\n\nmean(N::AbstractNormal) # mean vector\ncov(N::AbstractNormal)  # covariance matrix\nvar(N::AbstractNormal)  # vector of marginal variances\nstd(N::AbstractNormal)  # vector of marginal standard deviations\n\nresidual(N::AbstractNormal,x) # whitened residual of realisation x\nlogpdf(N::AbstractNormal,x)   # logarithm of the probability density function at x\nentropy(N::AbstractNormal)\nkldivergence(N1::AbstractNormal,N2::AbstractNormal)\nrand(N::AbstractNormal)","category":"page"},{"location":"tutorial_kalman_filter/#Implementing-a-Kalman-filter-and-a-Rauch-Tung-Striebel-smoother","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"","category":"section"},{"location":"tutorial_kalman_filter/","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"This tutorial describes how to implement a Kalman filter for the following state-space model","category":"page"},{"location":"tutorial_kalman_filter/","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"beginaligned\nx_0 sim mathcalN(mu_0 Sigma_0) \nx_n mid x_n-1 sim mathcalN(Phi  x_n-1 Q)\nz_n mid x_n sim mathcalN(Cx_nR)\nendaligned","category":"page"},{"location":"tutorial_kalman_filter/","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"given a measurement sequence z_0N.","category":"page"},{"location":"tutorial_kalman_filter/#Kalman-filter-implementation","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Kalman filter implementation","text":"","category":"section"},{"location":"tutorial_kalman_filter/","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"The classical imlementation of a Kalman filter is decomposed into a prediction step and an update step. These can be implemented as follows.","category":"page"},{"location":"tutorial_kalman_filter/","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"using MarkovKernels, LinearAlgebra, Plots\n\nfunction predict(N::AbstractNormal, K::NormalKernel{T,U,V}) where {T,U<:AbstractAffineMap,V}\n    N_new, B = invert(N, K)\n    return N_new, B\nend\n\nfunction update(\n    N::AbstractNormal,\n    L::Likelihood{NormalKernel{U,V,S},YT},\n) where {U,V<:AbstractAffineMap,S,YT}\n    M, C = invert(N, measurement_model(L))\n    y = measurement(L)\n    N_new = condition(C, y)\n    loglike = logpdf(M, y)\n\n    return N_new, M, loglike\nend","category":"page"},{"location":"tutorial_kalman_filter/","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"Note that the above implementation of the prediction step also computes the backward kernel required for the Rauch-Tung-Striebel smoother recursion. If no smoothing is to be performed, an alternative to the prediction step is simply to call marginalise, that is.","category":"page"},{"location":"tutorial_kalman_filter/","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"using MarkovKernels # hide\nfunction predict2(N::AbstractNormal, K::NormalKernel{T,U,V}) where {T,U<:AbstractAffineMap,V}\n    N_new = marginalise(N,K)\n    return N_new\nend","category":"page"},{"location":"tutorial_kalman_filter/","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"In any case, this tutorial does indeed demonstrate how to implement the smoother as well. The Kalman filter may thus be implemented by the following.","category":"page"},{"location":"tutorial_kalman_filter/","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"\nfunction kalman_filter(\n    ys::AbstractVecOrMat,\n    init::AbstractNormal,\n    fw_kernel::AbstractNormalKernel,\n    m_kernel::AbstractNormalKernel,\n)\n    N = size(ys, 1)\n\n    # initialise recursion\n    filter_distribution = init\n    filter_distributions = AbstractNormal[]      # filtering distributions\n    prediction_distributions = AbstractNormal[]  # one step-ahead measurement predictions\n    backward_kernels = AbstractNormalKernel[]    # backward kernels (used for rts smoothing)\n    loglike = 0.0\n\n    # create measurement model\n    y = ys[1, :]\n    likelihood = Likelihood(m_kernel, y)\n\n    # measurement update\n    filter_distribution, pred_distribution, loglike_increment =\n        update(filter_distribution, likelihood)\n\n    push!(prediction_distributions, pred_distribution)\n    loglike = loglike + loglike_increment\n    push!(filter_distributions, filter_distribution)\n\n    for n in 2:N\n\n        # predict\n        filter_distribution, bw_kernel = predict(filter_distribution, fw_kernel)\n        push!(backward_kernels, bw_kernel)\n\n        # create measurement model\n        y = ys[n, :]\n        likelihood = Likelihood(m_kernel, y)\n\n        # measurement update\n        filter_distribution, pred_distribution, loglike_increment =\n            update(filter_distribution, likelihood)\n\n        push!(filter_distributions, filter_distribution)\n        push!(prediction_distributions, pred_distribution)\n        loglike = loglike + loglike_increment\n    end\n\n    return filter_distributions, prediction_distributions, backward_kernels, loglike\nend","category":"page"},{"location":"tutorial_kalman_filter/","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"It remains to generate some data to try it out.","category":"page"},{"location":"tutorial_kalman_filter/","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"N = 2^6\nns = 0:N\n\n# define a Markov kernel for a homogeneous Markov proces\nλ = 0.5\n\ndt = 1.0\ndimx = 2\nΦ = [1.0 0; dt 1.0]\nQ = [dt dt^2/2; dt^2/2 dt^3/3]\nforward_kernel = NormalKernel(Φ, Q)\n\n# define initial distribution\ninit = Normal(zeros(dimx), 10.0*I(dimx))\n\n# sample Gauss-Markov process\nxs = rand(init, forward_kernel, N)\n\n# define output process\nC = [0.0 1.0]\noutput_kernel = DiracKernel(C)\n\n# define measurements of output process\nR = fill(150.0,1,1)\nmeasurement_kernel = NormalKernel(C,R)\n\n# sample outputs, measurements and plot\noutput = rand(output_kernel,xs)\nzs = rand(measurement_kernel,xs)\n\nfilter_distributions, prediction_distributions, backward_kernels, loglike =\n        kalman_filter(zs, init, forward_kernel, measurement_kernel)\n\n# plotting the filter state estimates\nplot(\n    ns,\n    xs,\n    layout=(dimx,1),\n    xlabel = [\"\" \"t\"],\n    label = [\"x0\" \"x1\"],\n    title = [\"Filter estimates of the state\" \"\"]\n)\nplot!(\n    ns,\n    filter_distributions,\n    layout=(dimx,1),\n    label = [\"x0filter\" \"x1filter\"]\n)","category":"page"},{"location":"tutorial_kalman_filter/","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"plot(\n    ns,\n    prediction_distributions,\n    xlabel = \"t\",\n    ylabel = \"y\",\n    label = \"one-step ahead prediction\",\n    title = \"One-step ahead predictions\"\n)\n\nscatter!(\n    ns,\n    zs,\n    label = \"measurements\",\n    color=\"black\"\n)","category":"page"},{"location":"tutorial_kalman_filter/#Implementing-the-Rauch-Tung-Striebel-smoother","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing the Rauch-Tung-Striebel smoother","text":"","category":"section"},{"location":"tutorial_kalman_filter/","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"Given the previous implementation of the Kalman filter, implementing the smoother is straight-forward.","category":"page"},{"location":"tutorial_kalman_filter/","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"function rts_recursion(\n    terminal::AbstractNormal,\n    kernels::AbstractVector{<:AbstractNormalKernel},\n)\n    N = length(kernels)\n    d = terminal\n    distributions = AbstractNormal[]\n\n    pushfirst!(distributions, d)\n\n    for n in 0:N-1\n        k = kernels[N-n]\n        d = marginalise(d, k)\n        pushfirst!(distributions, d)\n    end\n\n    return distributions\nend","category":"page"},{"location":"tutorial_kalman_filter/","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"The smoother estimates may thus be calculated from the previous output of the Kalman filter as follows.","category":"page"},{"location":"tutorial_kalman_filter/","page":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","title":"Implementing a Kalman filter and a Rauch-Tung-Striebel smoother","text":"smoother_distributions = rts_recursion(filter_distributions[end],backward_kernels)\n\nplot(\n    ns,\n    xs,\n    layout=(dimx,1),\n    xlabel = [\"\" \"t\"],\n    label = [\"x0\" \"x1\"],\n    title = [\"Filter estimates of the state\" \"\"]\n)\nplot!(\n    ns,\n    filter_distributions,\n    layout=(dimx,1),\n    label = [\"x0filter\" \"x1filter\"]\n)\nplot!(\n    ns,\n    smoother_distributions,\n    layout=(dimx,1),\n    label = [\"x0smoother\" \"x1smoother\"]\n)","category":"page"},{"location":"tutorial_pomp_sampling/#Sampling-from-Markov-realisable-processes","page":"Sampling from Probabilistic state-space models","title":"Sampling from Markov-realisable processes","text":"","category":"section"},{"location":"tutorial_pomp_sampling/","page":"Sampling from Probabilistic state-space models","title":"Sampling from Probabilistic state-space models","text":"This tutorial describes how to sample from the probabilistic state-space model given by","category":"page"},{"location":"tutorial_pomp_sampling/","page":"Sampling from Probabilistic state-space models","title":"Sampling from Probabilistic state-space models","text":"beginaligned\nx_0 sim mathcalN(mu_0 Sigma_0) \nx_n mid x_n-1 sim mathcalN(Phi  x_n-1 Q)\ny_n = C x_n\nendaligned","category":"page"},{"location":"tutorial_pomp_sampling/","page":"Sampling from Probabilistic state-space models","title":"Sampling from Probabilistic state-space models","text":"where x and y are referred to as the latent Gauss-Markov process and the output process, respectively. Additionally, noisy measurements of the output process will be generated according to","category":"page"},{"location":"tutorial_pomp_sampling/","page":"Sampling from Probabilistic state-space models","title":"Sampling from Probabilistic state-space models","text":"z_n mid x_n sim mathcalN(Cx_nR)","category":"page"},{"location":"tutorial_pomp_sampling/#Sampling-from-the-latent-Gauss-Markov-process","page":"Sampling from Probabilistic state-space models","title":"Sampling from the latent Gauss-Markov process","text":"","category":"section"},{"location":"tutorial_pomp_sampling/","page":"Sampling from Probabilistic state-space models","title":"Sampling from Probabilistic state-space models","text":"using MarkovKernels, LinearAlgebra, Plots\n\nN = 2^9\nns = 0:N\n\n# define a Markov kernel for a homogeneous Markov proces\nλ = 0.9\nσ = 1.0\ndimx = 2\nΦ = [λ 0.0; 1 - λ^2 λ]\nQ = (1-λ^2)*(1+λ^2) * 1.0*I(dimx)\nforward_kernel = NormalKernel(Φ, Q)\n\n# define initial distribution\ninit = Normal(zeros(dimx), 1.0*I(dimx))\n\n# sample Gauss-Markov process and plot\nxs = rand(init, forward_kernel, N)\nplot(\n    ns,\n    xs,\n    layout=(dimx,1),\n    xlabel = [\"\" \"t\"],\n    label = [\"x0\" \"x1\"],\n    title = [\"Latent Gauss-Markov process\" \"\"]\n)","category":"page"},{"location":"tutorial_pomp_sampling/#Sampling-output-and-measurements","page":"Sampling from Probabilistic state-space models","title":"Sampling output and measurements","text":"","category":"section"},{"location":"tutorial_pomp_sampling/","page":"Sampling from Probabilistic state-space models","title":"Sampling from Probabilistic state-space models","text":"# define output process\nC = σ*[1.0 -1.0]\noutput_kernel = DiracKernel(C)\n\n# define measurements of output process\nR = fill(0.1,1,1)\nmeasurement_kernel = NormalKernel(C,R)\n\n# sample outputs, measurements and plot\noutput = rand(output_kernel,xs)\nzs = rand(measurement_kernel,xs)\n\nplot(\n    ns,\n    output,\n    xlabel = \"t\",\n    ylabel = \"y\",\n    label = \"output process\",\n    title = \"Output process and measurements\"\n)\n\nscatter!(\n    ns,\n    zs,\n    label = \"measurements\",\n    color=\"black\"\n)","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = MarkovKernels\nusing MarkovKernels","category":"page"},{"location":"#MarkovKernels","page":"Home","title":"MarkovKernels","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for MarkovKernels.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [MarkovKernels]\nPages   = [\"normal.jl\",\"normal_generic.jl\",\"normalkernel.jl\",\"normalkernel_generic.jl\"]","category":"page"},{"location":"#MarkovKernels.AbstractNormal","page":"Home","title":"MarkovKernels.AbstractNormal","text":"AbstractNormal{T<:Number}\n\nAbstract type for representing normal distributed random vectors taking values in T.\n\n\n\n\n\n","category":"type"},{"location":"#MarkovKernels.Normal","page":"Home","title":"MarkovKernels.Normal","text":"Normal{T,U,V}\n\nStandard parametrisation of the normal distribution with element type T.\n\n\n\n\n\n","category":"type"},{"location":"#MarkovKernels.covp-Tuple{Normal}","page":"Home","title":"MarkovKernels.covp","text":"covp(N::Normal)\n\nReturns the internal representation of the covariance matrix of the Normal distribution N. For computing the actual covariance matrix, use cov.\n\n\n\n\n\n","category":"method"},{"location":"#MarkovKernels.dim-Tuple{Normal}","page":"Home","title":"MarkovKernels.dim","text":"dim(N::AbstractNormal)\n\nReturns the dimension of the normal distribution N.\n\n\n\n\n\n","category":"method"},{"location":"#MarkovKernels.entropy-Union{Tuple{AbstractNormal{T}}, Tuple{T}} where T","page":"Home","title":"MarkovKernels.entropy","text":"entropy(N::AbstractNormal)\n\nReturns the entropy of N.\n\n\n\n\n\n","category":"method"},{"location":"#MarkovKernels.kldivergence-Union{Tuple{T}, Tuple{AbstractNormal{T}, AbstractNormal{T}}} where T<:Number","page":"Home","title":"MarkovKernels.kldivergence","text":"kldivergence(N1::AbstractNormal,N2::AbstractNormal)\n\nReturns the Kullback-Leibler divergence between N1 and N2.\n\n\n\n\n\n","category":"method"},{"location":"#MarkovKernels.logpdf-Union{Tuple{T}, Tuple{AbstractNormal{T}, Any}} where T","page":"Home","title":"MarkovKernels.logpdf","text":"logpdf(N::AbstractNormal,x)\n\nReturns the logarithm of the probability density function of N evaluated at x.\n\n\n\n\n\n","category":"method"},{"location":"#MarkovKernels.residual-Tuple{AbstractNormal, Any}","page":"Home","title":"MarkovKernels.residual","text":"residual(N::AbstractNormal,x)\n\nReturns the whitened residual associated with N and observed vector x.\n\n\n\n\n\n","category":"method"},{"location":"#MarkovKernels.AbstractNormalKernel","page":"Home","title":"MarkovKernels.AbstractNormalKernel","text":"AbstractNormalKernel{T<:Number}\n\nAbstract type for representing Normal kernels taking values in T.\n\n\n\n\n\n","category":"type"},{"location":"#MarkovKernels.NormalKernel","page":"Home","title":"MarkovKernels.NormalKernel","text":"NormalKernel\n\nStandard parametrisation of Normal kernels.\n\n\n\n\n\n","category":"type"},{"location":"#MarkovKernels.condition-Tuple{AbstractNormalKernel, Any}","page":"Home","title":"MarkovKernels.condition","text":"condition(K::AbstractNormalKernel, x)\n\nReturns a Normal distribution corresponding to K evaluated at x.\n\n\n\n\n\n","category":"method"},{"location":"#MarkovKernels.covp-Tuple{NormalKernel}","page":"Home","title":"MarkovKernels.covp","text":"covp(K::NormalKernel)\n\nReturns the internal representation of the conditonal covariance matrix of the Normal kernel K. For computing the actual conditional covariance matrix, use cov.\n\n\n\n\n\n","category":"method"},{"location":"#MarkovKernels.invert-Union{Tuple{T}, Tuple{AbstractNormal{T}, AffineNormalKernel{T}}} where T","page":"Home","title":"MarkovKernels.invert","text":"invert(N::AbstractNorma, K::AffineNormalKernel)\n\nReturns the inverted factorisation of the joint distirbution P(y,x) = N(x)*K(y, x) i.e\n\nP(y,x) = Nout(y)*Kout(x,y)\n\n\n\n\n\n","category":"method"},{"location":"#MarkovKernels.marginalise-Union{Tuple{T}, Tuple{AbstractNormal{T}, AffineNormalKernel{T}}} where T","page":"Home","title":"MarkovKernels.marginalise","text":"marginalise(N::AbstractNormal, K::AffineNormalKernel)\n\nReturns M, K marginalised with respect to N i.e,\n\nM(y) = ∫ K(y,x)N(x) dx\n\n\n\n\n\n","category":"method"}]
}
